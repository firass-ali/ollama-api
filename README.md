# ğŸš€ Ollama Hugging Face API

## ğŸŒ Overview
This guide walks you through deploying an `LLM-based API` using Ollama on Hugging Face Spaces, ensuring seamless global accessibility and performance.

---

## ğŸ“Œ Deployment Steps

### 1ï¸âƒ£ Create a New Space
- Navigate to **Hugging Face â†’ Spaces â†’ New Space**.  
- Select **"Docker"** as the SDK.

### 2ï¸âƒ£ Clone the Space Repository
- Clone the repository to your local environment for easy file uploads.  
- Run the following command in your terminal:
  ```sh
  git clone your_space_url
  ```

### 3ï¸âƒ£ Upload Project Files
- Copy **`app.py`**, **`Dockerfile`**, and **`requirements.txt`** into your project folder.  
- The default model used is **`llama3.1`**, but you can modify it as needed.  
- If changing the model, update its name in both **`Dockerfile`** and **`app.py`**.  
- âš ï¸ Be mindful of computational constraintsâ€”Hugging Face Spaces primarily run on CPUs.

### 4ï¸âƒ£ Commit and Push Changes
- Commit and push your updates to the Hugging Face repository.
- The deployment process will **automatically build and launch** the Space.
- Monitor the **Build Logs** tab for any potential errors.

### 5ï¸âƒ£ Validate API Functionality
- Access your deployed API via:
  ```
  https://username-space_name.hf.space/docs
  ```
- Use `/` for a quick check and `/chat` to test LLM interactions.

---

## âœ… Best Practices & Considerations
âœ”ï¸ If experiencing **memory issues**, consider using a **lighter LLM model**.  
âœ”ï¸ Regularly check Hugging Face documentation for **resource updates and optimizations**.  
âœ”ï¸ Ensure **`requirements.txt`** includes all necessary dependencies to prevent build failures.  
âœ”ï¸ Implement **robust error handling** in `app.py` to improve API reliability.  

---

## ğŸ”— Repository Access  
Explore the API repository here: [ğŸš€ Ollama Hugging Face API](https://firass-ollama.hf.space/docs#/).